import argparse
import os
from pathlib import Path
import numpy as np
import torch
import torch.backends.cudnn as cudnn
import yaml
import torch
from omegaconf import OmegaConf
import open_clip as clip
import time

##############
from testfold.testllm import HuggingFaceLLM
import util.misc as misc


def load_config(config_path, display=False):
  config = OmegaConf.load(config_path)
  if display:
    print(yaml.dump(OmegaConf.to_container(config)))
  return config

def get_args_parser():
    parser = argparse.ArgumentParser("MAE pre-training", add_help=False)
    parser.add_argument(
        "--batch_size",
        default=64,
        type=int,
        help="Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus",
    )
    # Model parameters
    parser.add_argument("--llama_model_path", default="./llama", type=str, help="path of llama model")
    parser.add_argument("--model", default="llama7B", type=str, metavar="MODEL", help="Name of model to train")
    parser.add_argument("--max_seq_len", type=int, default=2048, metavar="LENGTH", help="the maximum sequence length")

    # Dataset parameters
    parser.add_argument("--output_dir", default="./output_dir", help="path where to save, empty for no saving")
    parser.add_argument("--device", default="cuda", help="device to use for training / testing")
    parser.add_argument("--seed", default=0, type=int)
    parser.add_argument("--resume", default="", help="resume from checkpoint")
    parser.add_argument("--num_workers", default=0, type=int)
    parser.add_argument(
        "--pin_mem",
        action="store_true",
        help="Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.",
    )
    parser.add_argument("--no_pin_mem", action="store_false", dest="pin_mem")
    parser.set_defaults(pin_mem=True)

    # distributed training parameters
    parser.add_argument("--world_size", default=1, type=int, help="number of distributed processes")
    parser.add_argument("--local_rank", default=-1, type=int)
    parser.add_argument("--dist_on_itp", action="store_true")
    parser.add_argument("--dist_url", default="env://", help="url used to set up distributed training")

    return parser


def main(args):

    texts = []
    misc.init_distributed_mode(args)

    print("=" * 60)
    print("ğŸš€ å¼€å§‹è¯æ±‡è¡¨æ‰©å±•ç¨‹åº")
    print("=" * 60)
    print("job dir: {}".format(os.path.dirname(os.path.realpath(__file__))))
    print("{}".format(args).replace(", ", ",\n"))

    device = torch.device(args.device)
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    seed = args.seed + misc.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    print(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")

    cudnn.benchmark = True
    

    ###Load CLIP
    model, _, preprocess = clip.create_model_and_transforms('ViT-L-14',pretrained='laion2b_s32b_b82k',device=device,cache_dir="/root/autodl-tmp/downloads")
    model.to(device)
    if args.distributed:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])#, find_unused_parameters=True)

    ###Load LLM using HuggingFace
    print("\nğŸ¤– æ­£åœ¨åŠ è½½LLMæ¨¡å‹...")
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {args.llama_model_path}")
    
    try:
        start_time = time.time()
        generator = HuggingFaceLLM(model_name=args.llama_model_path, device=device)
        load_time = time.time() - start_time
        print(f"âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ! è€—æ—¶: {load_time:.2f}ç§’")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {generator.tokenizer.vocab_size}")
    except Exception as e:
        print(f"âŒ LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    texts = []
    local_vocabularies = []

    print(f"\nğŸ”„ å¼€å§‹å¤„ç†è¯æ±‡è¡¨ (æ€»å…± {generator.tokenizer.vocab_size} ä¸ªè¯)")
    print("=" * 60)

    ###Iterating each Subwords
    total_tokens = generator.tokenizer.vocab_size
    start_process_time = time.time()
    
    for i in range(0, total_tokens):
        try:
            # è¿›åº¦æ˜¾ç¤º
            if i % 100 == 0 or i < 10:
                progress = (i / total_tokens) * 100
                elapsed_time = time.time() - start_process_time
                if i > 0:
                    avg_time_per_token = elapsed_time / i
                    eta = avg_time_per_token * (total_tokens - i)
                    print(f"ğŸ” å¤„ç†è¿›åº¦: {i}/{total_tokens} ({progress:.1f}%) | "
                          f"å·²ç”¨æ—¶: {elapsed_time:.1f}s | é¢„è®¡å‰©ä½™: {eta:.1f}s")
            
            # è§£ç å½“å‰token
            cur_token = generator.decode(i)
            
            # ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™æœ‰æ„ä¹‰çš„æ˜¾å¼å­—ç¬¦è¾“å‡º
            def should_skip_token(token):
                if not token:
                    return True
                    
                # è·³è¿‡ç‰¹æ®Štoken
                if token in generator.tokenizer.all_special_tokens:
                    return True
                
                # è·³è¿‡åªåŒ…å«ç©ºç™½å­—ç¬¦çš„token
                if not token.strip():
                    return True
                    
                # è·³è¿‡åŒ…å«æ§åˆ¶å­—ç¬¦çš„token
                if any(ord(c) < 32 for c in token if c not in [' ']):
                    return True
                    
                # è·³è¿‡åªåŒ…å«éå­—æ¯æ•°å­—å­—ç¬¦çš„tokenï¼ˆé™¤äº†ä¸€äº›å¸¸è§çš„æœ‰æ„ä¹‰ç¬¦å·ï¼‰
                meaningful_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
                if not any(c in meaningful_chars for c in token):
                    return True
                    
                return False
            
            if should_skip_token(cur_token):
                if i < 10:
                    print(f"âš ï¸  Token {i} è¢«è¿‡æ»¤ï¼Œè·³è¿‡å¤„ç†: {repr(cur_token)}")
                continue

            local_vocabularies.append(cur_token)
            
            if i < 5:  # åªæ‰“å°å‰5ä¸ªtokençš„è¯¦ç»†ä¿¡æ¯
                print(f"  ğŸ“ Token {i}: '{cur_token}' (é•¿åº¦: {len(cur_token)})")

            ###Generating Bigrams
            prompts = ["a photo of %s" % str(cur_token)]

            if i < 200:
                print(f"  ğŸ”® ç”ŸæˆäºŒå…ƒç»„ï¼Œæç¤ºè¯: {prompts[0]}")
            
            results = generator.text_completion(
                prompts,
                max_gen_len=1,
                temperature=0,
                top_p=1.0,
            )

            # æ·»åŠ ç»“æœæ£€æŸ¥
            if not results or len(results) == 0 or 'generation' not in results[0]:
                if i < 10:
                    print(f"âš ï¸  Token {i} äºŒå…ƒç»„ç”Ÿæˆå¤±è´¥æˆ–ç»“æœä¸ºç©º")
                continue
            
            bigram_result = results[0]['generation']
                
            if i < 200:
                print(f"  ğŸ“ˆ äºŒå…ƒç»„ç»“æœ: '{bigram_result}'")

            ###Generating Trigrams
            trigram_prompt = "a photo of %s" % str(cur_token + bigram_result)
            prompts = [trigram_prompt]
            
            if i < 200:
                print(f"  ğŸ”® ç”Ÿæˆä¸‰å…ƒç»„ï¼Œæç¤ºè¯: {trigram_prompt}")
            
            results_2 = generator.text_completion(
                prompts,
                max_gen_len=1,
                temperature=0,
                top_p=1.0,
            )
            
            if not results_2 or len(results_2) == 0 or 'generation' not in results_2[0]:
                if i < 10:
                    print(f"âš ï¸  Token {i} ä¸‰å…ƒç»„ç”Ÿæˆå¤±è´¥")
                continue
                
            trigram_result = results_2[0]['generation']
            if i < 200:
                print(f"  ğŸ“Š ä¸‰å…ƒç»„ç»“æœ: '{trigram_result}'")
            
            ###Save Vocabulary
            cur_cell = {"1": cur_token, "2": bigram_result, "3": trigram_result}
            texts.append(cur_cell)
            
            if i < 3:
                print(f"  ğŸ’¾ ä¿å­˜è¯æ±‡: {cur_cell}")
                print("  " + "-" * 50)
                
        except Exception as e:
            print(f"âŒ å¤„ç†Token {i} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            return

    total_process_time = time.time() - start_process_time
    print(f"\nâœ… è¯æ±‡è¡¨å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»å¤„ç†æ•°é‡: {len(texts)}")
    print(f"  - æœ¬åœ°è¯æ±‡æ•°é‡: {len(local_vocabularies)}")
    print(f"  - æ€»è€—æ—¶: {total_process_time:.2f}ç§’")
    print(f"  - å¹³å‡æ¯è¯è€—æ—¶: {total_process_time/len(texts):.3f}ç§’")

    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    
    try:
        ##Global Vocabulary
        global_vocab_path = "Subword_Bigram_Trigram_Vocabulary.npy"
        np.save(global_vocab_path, texts)
        print(f"âœ… å…¨å±€è¯æ±‡è¡¨å·²ä¿å­˜: {global_vocab_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(global_vocab_path)/1024/1024:.2f} MB")

        ##Local Vocabulary
        local_vocab_path = "local_vocabulary.npy"
        np.save(local_vocab_path, local_vocabularies)
        print(f"âœ… æœ¬åœ°è¯æ±‡è¡¨å·²ä¿å­˜: {local_vocab_path}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(local_vocab_path)/1024/1024:.2f} MB")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
    print("\n" + "=" * 60)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":

    args = get_args_parser()
    args = args.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)
